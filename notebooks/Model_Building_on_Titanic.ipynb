{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fce3f712-a5fd-4488-b1a2-56dd49982d3a",
   "metadata": {},
   "source": [
    "## Model Building on Titanic\n",
    "At this stage, as a data scientist: \n",
    "- I'll choose appropriate machine learning algorithms (e.g., linear regression, decision trees, random forest, XGBoost, neural networks).\n",
    "- Splits the dataset into training, validation, and testing sets.\n",
    "- Trains the model using the training data and tunes hyperparameters for better performance.\n",
    "- Evaluates the model using metrics (accuracy, precision, recall, RMSE, F1-score, AUC, etc.) depending on the problem type.\n",
    "\n",
    "For the titanic dataset, I will use the Logistic Regression.\n",
    "\n",
    "✅ Summary\n",
    "\n",
    "- LogisticRegression → algorithm used for Titanic classification.\n",
    "\n",
    "- cross_val_score → ensures stable evaluation with k-fold cross-validation.\n",
    "\n",
    "- GridSearchCV → finds the best hyperparameters for Logistic Regression.\n",
    "\n",
    "- Together, they make your Titanic model more accurate, robust, and reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18d50aaa-c01e-4fcd-870f-d286f1aaa342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2842dfb5-ae1c-4866-8c05-9f7415a1e569",
   "metadata": {},
   "source": [
    "**Load and Split the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d1db07d-34a3-485a-9723-38e0511bb5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Family_size</th>\n",
       "      <th>Cabin_status</th>\n",
       "      <th>Alone</th>\n",
       "      <th>Embarked_nan</th>\n",
       "      <th>Age_nan</th>\n",
       "      <th>...</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Title_Master</th>\n",
       "      <th>Title_Miss</th>\n",
       "      <th>Title_Mr</th>\n",
       "      <th>Title_Mrs</th>\n",
       "      <th>Title_Other</th>\n",
       "      <th>Age_scaled</th>\n",
       "      <th>Fare_log_scaled</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.888773</td>\n",
       "      <td>1.053619</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.232122</td>\n",
       "      <td>-0.159147</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.307868</td>\n",
       "      <td>0.828292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.232122</td>\n",
       "      <td>-0.227350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.535107</td>\n",
       "      <td>-0.533665</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass  Sex   Age  SibSp  Parch  Family_size  Cabin_status  Alone  \\\n",
       "0       1    1  54.0      0      0            1             1      1   \n",
       "1       3    1  26.0      0      0            1             0      1   \n",
       "2       2    1  25.0      1      2            3             0      0   \n",
       "3       3    1  26.0      1      0            1             0      1   \n",
       "4       3    0  22.0      0      0            1             0      1   \n",
       "\n",
       "   Embarked_nan  Age_nan  ...  Embarked_Q  Embarked_S  Title_Master  \\\n",
       "0             0        0  ...         0.0         1.0           0.0   \n",
       "1             0        1  ...         1.0         0.0           0.0   \n",
       "2             0        0  ...         0.0         0.0           0.0   \n",
       "3             0        0  ...         0.0         0.0           0.0   \n",
       "4             0        0  ...         0.0         1.0           0.0   \n",
       "\n",
       "   Title_Miss  Title_Mr  Title_Mrs  Title_Other  Age_scaled  Fare_log_scaled  \\\n",
       "0         0.0       1.0        0.0          0.0    1.888773         1.053619   \n",
       "1         0.0       1.0        0.0          0.0   -0.232122        -0.159147   \n",
       "2         0.0       1.0        0.0          0.0   -0.307868         0.828292   \n",
       "3         0.0       1.0        0.0          0.0   -0.232122        -0.227350   \n",
       "4         1.0       0.0        0.0          0.0   -0.535107        -0.533665   \n",
       "\n",
       "   Survived  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Family_size</th>\n",
       "      <th>Cabin_status</th>\n",
       "      <th>Alone</th>\n",
       "      <th>Embarked_nan</th>\n",
       "      <th>Age_nan</th>\n",
       "      <th>...</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Title_Master</th>\n",
       "      <th>Title_Miss</th>\n",
       "      <th>Title_Mr</th>\n",
       "      <th>Title_Mrs</th>\n",
       "      <th>Title_Other</th>\n",
       "      <th>Age_scaled</th>\n",
       "      <th>Fare_log_scaled</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.232122</td>\n",
       "      <td>-0.175318</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.146610</td>\n",
       "      <td>-0.535177</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.686599</td>\n",
       "      <td>-0.799211</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.747047</td>\n",
       "      <td>0.593927</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.141077</td>\n",
       "      <td>-0.470076</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass  Sex   Age  SibSp  Parch  Family_size  Cabin_status  Alone  \\\n",
       "0       3    1  26.0      1      1            2             0      0   \n",
       "1       2    1  31.0      0      0            1             0      1   \n",
       "2       3    1  20.0      0      0            1             0      1   \n",
       "3       2    0   6.0      0      1            3             0      0   \n",
       "4       3    0  14.0      1      0            2             0      0   \n",
       "\n",
       "   Embarked_nan  Age_nan  ...  Embarked_Q  Embarked_S  Title_Master  \\\n",
       "0             0        1  ...         0.0         0.0           1.0   \n",
       "1             0        0  ...         0.0         1.0           0.0   \n",
       "2             0        0  ...         0.0         1.0           0.0   \n",
       "3             0        0  ...         0.0         1.0           0.0   \n",
       "4             0        0  ...         0.0         0.0           0.0   \n",
       "\n",
       "   Title_Miss  Title_Mr  Title_Mrs  Title_Other  Age_scaled  Fare_log_scaled  \\\n",
       "0         0.0       0.0        0.0          0.0   -0.232122        -0.175318   \n",
       "1         0.0       1.0        0.0          0.0    0.146610        -0.535177   \n",
       "2         0.0       1.0        0.0          0.0   -0.686599        -0.799211   \n",
       "3         1.0       0.0        0.0          0.0   -1.747047         0.593927   \n",
       "4         1.0       0.0        0.0          0.0   -1.141077        -0.470076   \n",
       "\n",
       "   Survived  \n",
       "0         1  \n",
       "1         0  \n",
       "2         0  \n",
       "3         1  \n",
       "4         1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset\n",
    "train_df = pd.read_csv(r\"C:\\Users\\KOLADE\\OneDrive\\Documents\\Practices\\Titanic\\data\\Train.csv\")\n",
    "test_df = pd.read_csv(r\"C:\\Users\\KOLADE\\OneDrive\\Documents\\Practices\\Titanic\\data\\Test.csv\")\n",
    "\n",
    "display(train_df.head())\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e143a7a6-9c83-4d95-b4eb-7524f1f8bae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: ((596, 20), (596,))\n",
      "Test: ((295, 20), (295,))\n"
     ]
    }
   ],
   "source": [
    "X_train = train_df.iloc[:,:-1]\n",
    "y_train = train_df.iloc[:, -1]\n",
    "\n",
    "print(f\"Train: {X_train.shape, y_train.shape}\")\n",
    "\n",
    "X_test = test_df.iloc[:,:-1]\n",
    "y_test = test_df.iloc[:, -1]\n",
    "\n",
    "print(f\"Test: {X_test.shape, y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e5f170-1173-4e87-b46c-e99719034e8e",
   "metadata": {},
   "source": [
    "**Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "846dae36-bbac-47a5-80ca-7cad1f010036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived\n",
       "0    374\n",
       "1    222\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79f2ce01-a185-4aa8-8c3f-65eec5288f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.837248322147651\n",
      "Test Accuracy: 0.8406779661016949\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "print(\"Train Accuracy:\", log_reg.score(X_train, y_train))\n",
    "print(\"Test Accuracy:\", log_reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b63ad8d-4764-4eca-8665-ecf1dae61521",
   "metadata": {},
   "source": [
    "**Cross_Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4bf1e61-17f5-4539-9c62-1d9cf793fe67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.76666667 0.8907563  0.78991597 0.81512605 0.85714286]\n",
      "Cross-validation accuracy: 0.8239215686274509\n",
      "Cross-validation Std: 0.044905237084523264\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(log_reg, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(\"Scores:\", scores)\n",
    "print(\"Cross-validation accuracy:\", scores.mean())\n",
    "print(\"Cross-validation Std:\", scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba4a4fe-ad77-4f8b-b158-6a65e6bde99e",
   "metadata": {},
   "source": [
    "The scores are the accuracy values on each fold of the 5-fold cross-validation, they vary between 0.77 and 0.90. This shows the model is fairly stable (not wildly inconsistent across folds). Logistic Regression model correctly predicts survival about 82% of the time on unseen data (validation sets).\n",
    "\n",
    "Average performance across all folds shows that the Logistic Regression model correctly predicts survival about 82% of the time on unseen data (validation sets). This is the best estimate of generalization performance.\n",
    "\n",
    "Standard deviation tells you how much the accuracy varies between folds. Here it’s about ±4.5%, which is small → the model is stable across different splits of the data. If this number were large (e.g., ±0.12), it would suggest instability and sensitivity to how data is split.\n",
    "\n",
    "\n",
    "Compare the accuracy on the full training set to CV accuracy (82.2%):\n",
    "\n",
    ">>\n",
    "Train Accuracy = 83.7%  \n",
    "CV Accuracy = 82.2%  \n",
    "Difference = about 1.5%\n",
    "\n",
    "👉 Interpretation: \n",
    "- The model is not overfitting (train accuracy isn’t much higher than CV accuracy).\n",
    "- It’s also not underfitting (scores are well above random guessing).\n",
    "- Logistic Regression is generalizing well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3f5049-1caf-4cf3-b0bc-5bd883c43c3f",
   "metadata": {},
   "source": [
    "- Logistic Regression achieves ~82% accuracy, stable with low variance.\n",
    "\n",
    "- Train vs CV accuracy is very close → good generalization.\n",
    "\n",
    "- This makes Logistic Regression a solid baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b72867d-a66f-4d0f-8b4d-9dfb06623840",
   "metadata": {},
   "source": [
    "**🔹 What To Do Next**\n",
    "\n",
    "Now that you’ve established a baseline:\n",
    "1. Hyperparameter tuning: Use GridSearchCV to see if tweaking C, penalty, and solver improves accuracy.\n",
    "\n",
    "2. Try other models: RandomForest, GradientBoosting, or XGBoost, and compare CV scores with Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41087e5e-bb5a-4ef4-80e8-6b2111ff417a",
   "metadata": {},
   "source": [
    "**Hyperparameter tuning: Using GridSearchCV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fa6cd2a-15e6-4ce9-8db6-bcacc05da86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Best CV Score: 0.8289495798319327\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid\n",
    "param_grid = [\n",
    "    {'penalty': ['l1'], 'solver': ['liblinear', 'saga'], 'C': [1e-4, 1e-3, 0.01, 0.1, 1, 10, 50]},\n",
    "    {'penalty': ['l2'], 'solver': ['liblinear', 'saga'], 'C': [1e-4, 1e-3, 0.01, 0.1, 1, 10, 50]},\n",
    "]\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(LogisticRegression(max_iter=3000), param_grid,\n",
    "                           cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best CV Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "978a9ef2-e4fe-4ba2-a504-1471ad05eba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.837248322147651\n",
      "Test Accuracy: 0.8406779661016949\n"
     ]
    }
   ],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "print(\"Train Accuracy:\", best_model.score(X_train, y_train))\n",
    "print(\"Test Accuracy:\", best_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc31429-1a28-4364-b57f-26b3ee0b8654",
   "metadata": {},
   "source": [
    "✅ Summary of Findings\n",
    "- Logistic Regression with L1 regularization (penalty='l1') and C=10 is the best choice for Titanic data.\n",
    "\n",
    "- Your CV accuracy is ~83%, which is strong for this dataset.\n",
    "\n",
    "- You can now safely evaluate on the test set and then decide if you want to explore more powerful models like Random Forest or XGBoost for better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6c30ecf-ed9f-4b9e-998d-8be74a462e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8407\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.87       175\n",
      "           1       0.81      0.80      0.80       120\n",
      "\n",
      "    accuracy                           0.84       295\n",
      "   macro avg       0.84      0.83      0.83       295\n",
      "weighted avg       0.84      0.84      0.84       295\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      "[[152  23]\n",
      " [ 24  96]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = log_reg.predict(X_test)\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred):.4f}\\n\")\n",
    "\n",
    "print(f\"Classification Report: \\n{classification_report(y_test, y_pred)}\\n\")\n",
    "\n",
    "print(f\"Confusion Matrix: \\n{confusion_matrix(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f2b39b-a17a-4749-9957-3ff5caadfbbb",
   "metadata": {},
   "source": [
    "**🔎 First Model (Default Logistic Regression)**\n",
    "\n",
    "Accuracy: 0.8407 (~84%)\n",
    "Class 0 (Died):\n",
    "- Correctly predicted 152 out of 175 (87% recall).\n",
    "- 23 were misclassified as survivors (false positives).\n",
    "\n",
    "Class 1 (Survived):\n",
    "- Correctly predicted 96 out of 120 (80% recall).\n",
    "- 24 actual survivors were missed (false negatives).\n",
    "\n",
    "Precision / Recall Trade-off:\n",
    "- Precision for survivors (1) = 0.81 → when model says \"survived,\" it’s correct ~81% of the time.\n",
    "- Recall for survivors (1) = 0.80 → model detects ~80% of all survivors.\n",
    "- F1-score = 0.80 → balanced but not perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e52a8e2b-1993-4f25-9817-ef54b12ae1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8406779661016949\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.88      0.87       175\n",
      "           1       0.82      0.78      0.80       120\n",
      "\n",
      "    accuracy                           0.84       295\n",
      "   macro avg       0.84      0.83      0.83       295\n",
      "weighted avg       0.84      0.84      0.84       295\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      "[[154  21]\n",
      " [ 26  94]]\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = best_model.predict(X_test)\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred2)}\\n\")\n",
    "\n",
    "print(f\"Classification Report: \\n{classification_report(y_test, y_pred2)}\\n\")\n",
    "\n",
    "print(f\"Confusion Matrix: \\n{confusion_matrix(y_test, y_pred2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8047ab52-9b78-45fc-955d-3d21cdd59a21",
   "metadata": {},
   "source": [
    "**🔎 Tuned Model (Best Parameters via GridSearch)**\n",
    "\n",
    "Accuracy: 0.8407 (same ~84%)\n",
    "\n",
    "Class 0 (Died):\n",
    "- 154 correct (88% recall, slightly better).\n",
    "- 21 misclassified as survivors (improvement over 23).\n",
    "\n",
    "Class 1 (Survived):\n",
    "- 94 correct (78% recall, slightly worse).\n",
    "- 26 missed (more false negatives than first model).\n",
    "\n",
    "Precision / Recall Trade-off:\n",
    "- Precision for survivors = 0.82 (slightly better than 0.81).\n",
    "- Recall for survivors = 0.78 (slightly worse than 0.80).\n",
    "- F1-score stays at 0.80."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c313755-8f1a-4479-a661-1c6557a1de08",
   "metadata": {},
   "source": [
    "**⚖️ Model Comparison**\n",
    "\n",
    "- Both models give the same overall accuracy (84%).\n",
    "- Base model is a little better at catching survivors (higher recall for class 1).\n",
    "- Tuned model is a little better at avoiding false positives (higher precision for class 1).\n",
    "\n",
    "So the choice depends on your goal:\n",
    "\n",
    "- If you care more about not missing survivors → base model is slightly better.\n",
    "- If you care more about being confident when predicting survivors → tuned model is slightly better.\n",
    "\n",
    "✅ Is the Model Good Enough? Yes, for Titanic data this is quite good:\n",
    "\n",
    "- Accuracy > 80% is solid for a relatively small dataset.\n",
    "\n",
    "- Balanced precision/recall across classes.\n",
    "\n",
    "- No serious class imbalance issue (175 died vs. 120 survived).\n",
    "\n",
    "But:\n",
    "\n",
    "- It still misses ~20% of survivors (false negatives), which may or may not be acceptable depending on the context.\n",
    "\n",
    "- Further improvements could come from feature engineering (e.g., extracting titles from names, family size, cabin deck) rather than just tuning logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b71189b9-8b77-41c9-9f4c-4f6c45538c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.8968095238095237\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_proba = log_reg.predict_proba(X_test)[:, 1]\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74f51fc5-0bed-4580-b44e-f64f966a3235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.8969047619047619\n"
     ]
    }
   ],
   "source": [
    "y_proba2 = best_model.predict_proba(X_test)[:, 1]\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a2fbaf-942b-41b1-9c64-c6c5f2eca631",
   "metadata": {},
   "source": [
    "### Train Multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8ffa0f8-91db-414f-aee7-b411e0ced1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf9536d4-3254-4767-8dcf-204ac1562d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Logistic Regression': LogisticRegression(max_iter=3000), 'Naive Bayes': GaussianNB(), 'KNN': KNeighborsClassifier(), 'Decision Tree': DecisionTreeClassifier(random_state=42), 'Random Forest': RandomForestClassifier(random_state=42)}\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=3000),\n",
    "        \"Naive Bayes\": GaussianNB(),\n",
    "        \"KNN\": KNeighborsClassifier(),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "        \"Random Forest\": RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d40ef616-6a33-4954-b2cf-9abacae0353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(model, X_train_df, y_train_df, X_test_df, y_test_df):\n",
    "    model.fit(X_train_df, y_train_df)\n",
    "    y_pred = model.predict(X_test_df)\n",
    "    y_proba = model.predict_proba(X_test_df)[:, 1]\n",
    "\n",
    "    print(f\"Train Accuracy: {model.score(X_train_df, y_train_df)}\")\n",
    "    print(f\"Test Accuracy: {accuracy_score(y_test_df, y_pred)}\")\n",
    "    print(\"ROC-AUC:\", roc_auc_score(y_test_df, y_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2c804b1-8b7b-4866-9e60-e61255e27166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "Train Accuracy: 0.837248322147651\n",
      "Test Accuracy: 0.8406779661016949\n",
      "ROC-AUC: 0.8968095238095237\n",
      "**************************************************\n",
      "Model: Naive Bayes\n",
      "Train Accuracy: 0.8003355704697986\n",
      "Test Accuracy: 0.7864406779661017\n",
      "ROC-AUC: 0.8696190476190476\n",
      "**************************************************\n",
      "Model: KNN\n",
      "Train Accuracy: 0.8338926174496645\n",
      "Test Accuracy: 0.7932203389830509\n",
      "ROC-AUC: 0.8708333333333333\n",
      "**************************************************\n",
      "Model: Decision Tree\n",
      "Train Accuracy: 0.9832214765100671\n",
      "Test Accuracy: 0.7254237288135593\n",
      "ROC-AUC: 0.7122142857142857\n",
      "**************************************************\n",
      "Model: Random Forest\n",
      "Train Accuracy: 0.9832214765100671\n",
      "Test Accuracy: 0.8101694915254237\n",
      "ROC-AUC: 0.8892380952380953\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    print(f\"Model: {name}\")\n",
    "    evaluate_models(model, X_train, y_train, X_test, y_test)\n",
    "    print(\"**\" * 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11bd741-c9d4-45a2-8dca-f4393ae23d34",
   "metadata": {},
   "source": [
    "**Hyperparameters Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af12470d-7d1c-45e1-ad29-2f05f224ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "knn_params = {\n",
    "    \"n_neighbors\": [3, 5, 7, 9, 11],\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "    \"p\": [1, 2]   # 1 = Manhattan, 2 = Euclidean\n",
    "}\n",
    "\n",
    "# Decision Tree\n",
    "dt_params = {\n",
    "    \"max_depth\": [3, 5, 7, 10, None],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"criterion\": [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "# Random Forest\n",
    "rf_params = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_depth\": [None, 5, 10],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"bootstrap\": [True, False]\n",
    "}\n",
    "\n",
    "# KNN GridSearch\n",
    "knn_grid = GridSearchCV(KNeighborsClassifier(),\n",
    "                        param_grid=knn_params,\n",
    "                        cv=5,\n",
    "                        scoring=\"roc_auc\",\n",
    "                        n_jobs=-1)\n",
    "\n",
    "# Decision Tree GridSearch\n",
    "dt_grid = GridSearchCV(DecisionTreeClassifier(random_state=42),\n",
    "                       param_grid=dt_params,\n",
    "                       cv=5,\n",
    "                       scoring=\"roc_auc\",\n",
    "                       n_jobs=-1)\n",
    "\n",
    "# Random Forest GridSearch\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42),\n",
    "                       param_grid=rf_params,\n",
    "                       cv=5,\n",
    "                       scoring=\"roc_auc\",\n",
    "                       n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02c86409-fba1-4b5e-9351-4372991df6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Tuning KNN ...\n",
      "Best Params: {'n_neighbors': 11, 'p': 1, 'weights': 'uniform'}\n",
      "Best CV Score: 0.8187104377104376\n",
      "Train Accuracy: 0.8205\n",
      "Test Accuracy: 0.7966\n",
      "Test Accuracy: 0.7966\n",
      "ROC-AUC: 0.8714\n",
      "**************************************************\n",
      "🔎 Tuning Decision Tree ...\n",
      "Best Params: {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "Best CV Score: 0.8281949221949223\n",
      "Train Accuracy: 0.8557\n",
      "Test Accuracy: 0.7898\n",
      "Test Accuracy: 0.7898\n",
      "ROC-AUC: 0.8390\n",
      "**************************************************\n",
      "🔎 Tuning Random Forest ...\n",
      "Best Params: {'bootstrap': True, 'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Best CV Score: 0.8551748293748294\n",
      "Train Accuracy: 0.8758\n",
      "Test Accuracy: 0.8203\n",
      "Test Accuracy: 0.8203\n",
      "ROC-AUC: 0.9000\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "grids = {\n",
    "    \"KNN\": knn_grid,\n",
    "    \"Decision Tree\": dt_grid,\n",
    "    \"Random Forest\": rf_grid\n",
    "}\n",
    "\n",
    "for name, grid in grids.items():\n",
    "    print(f\"🔎 Tuning {name} ...\")\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid.best_estimator_\n",
    "    y_predict = best_model.predict(X_test)\n",
    "    y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(f\"Best Params: {grid.best_params_}\")\n",
    "    print(\"Best CV Score:\", grid.best_score_)\n",
    "    print(f\"Train Accuracy: {best_model.score(X_train, y_train):.4f}\")\n",
    "    print(f\"Test Accuracy: {best_model.score(X_test, y_test):.4f}\")\n",
    "    print(f\"Test Accuracy: {accuracy_score(y_test, y_predict):.4f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "    print(\"**\" * 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f09d9b-7c26-4cc1-9cc1-7a653d96bdd8",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "**From the baseline model evaluations:**  \n",
    "- Logistic Regression achieved a strong balance with 84% test accuracy and a ROC-AUC of ~0.897, making it a solid baseline.\n",
    "- Naive Bayes and KNN performed reasonably but had slightly lower test accuracy (~79%) and ROC-AUC compared to Logistic Regression.\n",
    "- Decision Tree showed signs of overfitting (very high training accuracy ≈ 98% but low test accuracy ≈ 72%).\n",
    "- Random Forest achieved a good balance with 81% test accuracy and a ROC-AUC of ~0.889, but still slightly below Logistic Regression.\n",
    "\n",
    "**After Hyperparameter Tuning:**  \n",
    "- KNN improved slightly but remained at ~79% test accuracy and ROC-AUC ≈ 0.871.\n",
    "- Decision Tree generalized better after tuning (≈79% test accuracy, ROC-AUC ≈ 0.839), but still weaker than ensemble methods.\n",
    "- Random Forest became the best overall performer with 82% test accuracy and the highest ROC-AUC (0.90) after tuning, showing strong predictive power while avoiding overfitting.\n",
    "\n",
    "**✅ Summary:**  \n",
    "\n",
    "1. Random Forest (tuned) provided the best overall results in terms of accuracy and ROC-AUC.\n",
    "\n",
    "2. Logistic Regression remained a strong and interpretable baseline model.\n",
    "\n",
    "3. Ensemble methods (like Random Forest) proved to be more robust than single-tree models and distance-based algorithms (KNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb51bcfc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
